{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking pre-trained VGG model and\n",
    "# implementing by importing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path =\"tiger.jpg\"\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the VGG16 model shipped with Keras. \n",
    "# And loading certain helper functions required and preprocessing our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x)\n",
    "class_idx = np.argmax(preds[0])\n",
    "class_output = model.output[:, class_idx]\n",
    "last_conv_layer = model.get_layer(\"block5_conv3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing our model and loading the image.\n",
    "The VGG network expects input size to be (224×224×3), so we resize our image to the required size.\n",
    "Since, we are passing only one image through the network, it’s required to expand the first dimension noting it as a batch of size 1. We then normalize our image by subtracting mean RGB values from the input image using a helper function preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = decode_predictions(preds)[0][0]\n",
    "print('Predicted class: %s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the map for the top prediction. So, we get the predictions for the image and then, we take the topmost class index. Remember that we can compute map for any class. Then, we take the output from the final convolutional layer in the VGG16 which isblock5_conv3. The resulting feature map will be of shape 14×14×512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "for i in range(512):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute the gradient of the class output value with respect to the feature map. Then, we pool the gradients over all the axes leaving out the channel dimension. Finally, we weigh the output feature map with the computed gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging the weighed feature map along the channel dimension resulting in a heat map of size 14 times 14. And, then we normalize the heat map to make the values in between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original\")\n",
    "plt.show()\n",
    "plt.title(\"GradCam\")\n",
    "plt.imshow(superimposed_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use OpenCV to read the image, resize the existing heatmap to the image size. We blend the original image and the heatmap to superimpose the heatmap on to the image. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
